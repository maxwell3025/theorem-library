**Knowledge Goal Number & Name:** \[KG1\] Endpoint Definitions

**File Reference:**
`verification-service/Dockerfile:24-42`

**Code Fragment:**
```
@app.get("/health", response_model=model.HealthCheckResponse)
async def health_check(x_correlation_id: str = fastapi.Header()) -> fastapi.Response:
    # Currently, nothing can cause a service to report itself as unhealthy
    status: common.model.HealthCheckStatus = "healthy"
    status_code = 200 if status == "healthy" else 503
    # Use methods defined in /common to run health checks
    dependencies: typing.Dict[str, common.model.HealthCheckDependency] = {
        "redis": common.api.redis.check_health(),
    }

    response_content = model.HealthCheckResponse(
        status=status,
        dependencies=dependencies,
    )

    return fastapi.Response(
        content=response_content.model_dump_json(exclude_none=True),
        status_code=status_code,
    )
```

**Justification:**
This is the definition of the healthcheck endpoint for the verification service.
This code fragment is an example of the standard way of defining API endpoints with FastAPI.
It also shows that I understand status codes.

---

**Knowledge Goal Number & Name:** \[KG2\] Containerization

**File Reference:**
`verification-service/Dockerfile:1-11`

**Code Fragment:**
```
FROM python:3.11-slim

WORKDIR /app

COPY ./requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY ./app .
COPY --from=common . ./common

ENV PYTHONUNBUFFERED=1
```

**Justification:**
This Dockerfile defines the build process for the `verification-service` microservice.
By setting the base image, installing dependencies, and defining the startup command, it bundles the application code and environment into a single, isolated, and portable container image, which is the definition of Containerization.
This also shows that I know how to modify Dockerfiles to add the dependencies that I want, since I customize the copy flag, and omit the command, since it is split into a worker and API server.

---

**Knowledge Goal Number & Name:** \[KG3 & KG6\] Container Orchestration & Maintainability

**File Reference:** scripts/generate_compose.py:24-70

**Code Fragment:**
```
def main():
    parser = argparse.ArgumentParser(
        description="Generate docker-compose.yml from common/config.py"
    )

    parser.add_argument(
        "--output",
        "-o",
        default="docker-compose.yml",
        help="Output file path (default: docker-compose.yml)",
    )

    args = parser.parse_args()
    output_path = Path(args.output)

    content = compose_specification.model_dump(exclude_none=True, by_alias=True)

    def represent_enum_as_string(dumper: yaml.SafeDumper, data: Condition):
        return dumper.represent_str(data.value)

    yaml.SafeDumper.add_representer(Condition, represent_enum_as_string)

    # Source - https://stackoverflow.com/a/41786451
    # Posted by Jace Browning, modified by community. See post 'Timeline' for change history
    # Retrieved 2025-12-12, License - CC BY-SA 4.0

    def represent_none(dumper: yaml.SafeDumper, _):
        return dumper.represent_scalar("tag:yaml.org,2002:null", "")

    yaml.add_representer(type(None), represent_none)

    yaml.SafeDumper.ignore_aliases = lambda self, data: True

    # Add warning comment to the generated file
    warning_comment = (
        "# This file is auto-generated by scripts/generate_compose.py\n"
        "# Do not edit this file directly.\n"
        "# Any changes should be made to common/config.py, and this file should be generated by running:\n"
        "#   python scripts/generate_compose.py\n"
        "\n"
    )

    output_path.write_text(warning_comment + yaml.safe_dump(content))

    print(f"Successfully generated {output_path}")

    return 0
```

**Justification:**
This is part of a script that I use to generate `docker-compose.yml`.
I wrote this script since I noticed a lot of duplication in my docker-compose that could not be mitigated with `.env` entries.
This shows that I understand the structure of the specification well.
This also makes the system more maintainable, since it reduces code duplication and makes the configuration more accessible to the service code, since it is written in Python and can be imported.
If given more time, I would probably investigate how to do this with Kubernetes.

---

**Knowledge Goal Number & Name:** \[KG4 & KG5\] API Gateway & Load Balancing

**File Reference:** nginx/nginx.conf:1-57

**Code Fragment:**
```
upstream dependency-service {
    server dependency-service:8000;
}

upstream verification-service {
    server verification-service:8000;
}

upstream pdf-service {
    server pdf-service:8000;
}

upstream latex-service {
    server latex-service:8000;
}

log_format custom_format '[$time_iso8601][nginx               ][INFO] $remote_addr "$request" status=$status bytes=$body_bytes_sent';

access_log /dev/stdout custom_format;
access_log /dev/stderr custom_format;
error_log /dev/stderr info;

server {
    listen 80;
    
    location /dependency-service/ {
        proxy_pass http://dependency-service/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Correlation-ID $request_id;
    }

    location /verification-service/ {
        proxy_pass http://verification-service/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Correlation-ID $request_id;
    }

    location /pdf-service/ {
        proxy_pass http://pdf-service/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Correlation-ID $request_id;
    }

    location /latex-service/ {
        proxy_pass http://latex-service/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Correlation-ID $request_id;
    }
}
```

**Justification:**
This is the configuration for Nginx, which is my API gateway. 
This configures Nginx to forward all external requests to the correct servers, showing that I understand one of the purposes of an API gateway and know how to configure one in practice.
I also set the the upstream servers to point to just the service names, so the Docker DNS will automatically distribute these requests across all replicas.
This shows that I know how to setup my architecture to do load balancing.

---

**Knowledge Goal Number & Name:** \[KG6\] Reliability, Scalability, and Maintainability

**File Reference:** `rabbitmq/definitions.json:1:35`

**Code Fragment:**
```
{
    "users": [
        {
            "name": "guest",
            "password": "guest",
            "tags": "administrator"
        }
    ],
    "vhosts": [
        {
            "name": "/"
        }
    ],
    "permissions": [
        {
            "user": "guest",
            "vhost": "/",
            "configure": ".*",
            "write": ".*",
            "read": ".*"
        }
    ],
    "policies": [
        {
            "vhost": "/",
            "name": "max-queue-length",
            "pattern": ".*",
            "apply-to": "queues",
            "definition": {
                "max-length": 3,
                "overflow": "reject-publish"
            }
        }
    ]
}
```

**Justification:**
TODO

---

**Knowledge Goal Number & Name:** \[KG7\] Service Communication

**File Reference:** TODO

**Code Fragment:**
```
TODO
```

**Justification:**
TODO
